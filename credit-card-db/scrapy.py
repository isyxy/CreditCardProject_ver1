import time
import os
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import re

def setup_driver():
    """è¨­å®šChromeé©…å‹•å™¨"""
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # å¦‚æœä¸éœ€è¦é¡¯ç¤ºç€è¦½å™¨è¦–çª—ï¼Œå¯ä»¥å–æ¶ˆè¨»è§£
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    return webdriver.Chrome(options=chrome_options)

def scrape_card_data(url, card_name, wait_time=6):
    """é€šç”¨çˆ¬å–å‡½æ•¸"""
    driver = setup_driver()
    try:
        print(f"ğŸ” æ­£åœ¨çˆ¬å–: {card_name}")
        driver.get(url)
        time.sleep(wait_time)
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for tag in soup(["script", "style", "noscript", "nav", "footer", "header"]):
            tag.decompose()
        
        # ç§»é™¤å»£å‘Šå’Œä¸ç›¸é—œå€å¡Š
        for ad in soup.find_all(class_=lambda x: x and any(keyword in x.lower() for keyword in ['ad', 'banner', 'menu', 'sidebar'])):
            ad.decompose()
        
        # ç²å–æ–‡å­—å…§å®¹
        all_text = soup.get_text(separator="\n", strip=True)
        
        # æ¸…ç†æ–‡å­—
        all_lines = []
        for line in all_text.split('\n'):
            line = line.strip()
            if line and len(line) > 1 and not line.isspace():
                all_lines.append(line)
        
        if not all_lines:
            all_lines.append(f"âš ï¸ ç„¡æ³•æ“·å–å…§å®¹: {url}")
        
        print(f"âœ… æˆåŠŸçˆ¬å– {card_name}: {len(all_lines)} è¡Œå…§å®¹")
        return all_lines
        
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±æ•— {card_name}: {e}")
        return [f"éŒ¯èª¤: ç„¡æ³•çˆ¬å– {card_name}", f"ç¶²å€: {url}", f"éŒ¯èª¤è¨Šæ¯: {str(e)}"]
    finally:
        driver.quit()


# =============================================================================
# å°æ–°éŠ€è¡Œä¿¡ç”¨å¡å‡½æ•¸
# =============================================================================

def scrape_taishin_card():
    url = "https://www.taishinbank.com.tw/TSB/personal/credit/intro/overview/cg047/card001/"
    return scrape_card_data(url, "å°æ–°ä¿¡ç”¨å¡", wait_time=8)

# =============================================================================
# æ°¸è±éŠ€è¡Œä¿¡ç”¨å¡å‡½æ•¸
# =============================================================================

def scrape_sinopac_dawho_card():
    """æ°¸è±éŠ€è¡Œ - DAWHOå¡"""
    url = "https://bank.sinopac.com/sinopacBT/personal/credit-card/introduction/bankcard/DAWHO.html"
    return scrape_card_data(url, "DAWHOå¡", wait_time=6)

def scrape_sinopac_daway_card():
    """æ°¸è±éŠ€è¡Œ - DAWAYå¡"""
    url = "https://bank.sinopac.com/sinopacBT/personal/credit-card/introduction/bankcard/DAWAY.html"
    return scrape_card_data(url, "DAWAYå¡", wait_time=6)

# =============================================================================
# åŒ¯è±éŠ€è¡Œä¿¡ç”¨å¡å‡½æ•¸
# =============================================================================

def scrape_hsbc_cashback_card():
    """åŒ¯è±éŠ€è¡Œ - ç¾é‡‘å›é¥‹å¾¡ç’½å¡"""
    url = "https://www.hsbc.com.tw/credit-cards/products/cashback-titanium/"
    return scrape_card_data(url, "ç¾é‡‘å›é¥‹å¾¡ç’½å¡", wait_time=8)

def scrape_hsbc_liveplus_card():
    """åŒ¯è±éŠ€è¡Œ - LivePlusæ‚ æ´»å¡"""
    url = "https://www.hsbc.com.tw/credit-cards/products/liveplus/"
    return scrape_card_data(url, "LivePlusæ‚ æ´»å¡", wait_time=8)

# =============================================================================
# ç‰å±±éŠ€è¡Œä¿¡ç”¨å¡å‡½æ•¸
# =============================================================================

def scrape_esun_pi_card():
    """ç‰å±±éŠ€è¡Œ - PiéŒ¢åŒ…ä¿¡ç”¨å¡"""
    url = "https://www.esunbank.com/zh-tw/personal/credit-card/intro/co-branded-card/pi-card"
    return scrape_card_data(url, "PiéŒ¢åŒ…ä¿¡ç”¨å¡", wait_time=8)

def scrape_esun_unicard():
    """ç‰å±±éŠ€è¡Œ - U Bearå¡"""
    url = "https://www.esunbank.com/zh-tw/personal/credit-card/intro/bank-card/unicard"
    return scrape_card_data(url, "U Bearå¡", wait_time=8)

def scrape_esun_unicard_top100():
    """ç‰å±±éŠ€è¡Œ - U Bearå¡ç™¾å¤§åº—å®¶"""
    url = "https://event.esunbank.com.tw/credit/unicard/discount-channel.html"
    driver = setup_driver()
    try:
        print(f"ğŸ” æ­£åœ¨çˆ¬å–: U Bearå¡ç™¾å¤§åº—å®¶")
        driver.get(url)
        time.sleep(8)
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for tag in soup(["script", "style", "noscript", "nav", "footer", "header"]):
            tag.decompose()
        
        # ç²å–æ–‡å­—å…§å®¹
        all_text = soup.get_text(separator="\n", strip=True)
        
        # æ¸…ç†æ–‡å­—
        all_lines = ["\n========== U Bearå¡ç™¾å¤§åº—å®¶å„ªæƒ  ==========\n"]
        for line in all_text.split('\n'):
            line = line.strip()
            if line and len(line) > 1 and not line.isspace():
                all_lines.append(line)
        
        all_lines.append("\n========== ç™¾å¤§åº—å®¶è³‡æ–™çµæŸ ==========\n")
        
        if len(all_lines) <= 2:
            all_lines.append(f"âš ï¸ ç„¡æ³•æ“·å–ç™¾å¤§åº—å®¶å…§å®¹: {url}")
        
        print(f"âœ… æˆåŠŸçˆ¬å– U Bearå¡ç™¾å¤§åº—å®¶: {len(all_lines)} è¡Œå…§å®¹")
        return all_lines
        
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±æ•— U Bearå¡ç™¾å¤§åº—å®¶: {e}")
        return [f"éŒ¯èª¤: ç„¡æ³•çˆ¬å– U Bearå¡ç™¾å¤§åº—å®¶", f"ç¶²å€: {url}", f"éŒ¯èª¤è¨Šæ¯: {str(e)}"]
    finally:
        driver.quit()

def scrape_esun_kumamon_card():
    """ç‰å±±éŠ€è¡Œ - ç†Šæœ¬ç†Šä¿¡ç”¨å¡"""
    url = "https://www.esunbank.com/zh-tw/personal/credit-card/intro/bank-card/kumamon_card"
    return scrape_card_data(url, "ç†Šæœ¬ç†Šä¿¡ç”¨å¡", wait_time=8)

# =============================================================================
# åœ‹æ³°ä¸–è¯éŠ€è¡Œä¿¡ç”¨å¡å‡½æ•¸
# =============================================================================

def scrape_cathay_cube_card():
    """åœ‹æ³°ä¸–è¯éŠ€è¡Œ - CUBEå¡"""
    url = "https://www.cathay-cube.com.tw/cathaybk/personal/product/credit-card/cards/cube"
    return scrape_card_data(url, "CUBEå¡", wait_time=10)

# =============================================================================
# éŠ€è¡Œè³‡æ–™æ•´åˆå‡½æ•¸
# =============================================================================

def save_bank_file(bank_name, cards_data, filename):
    """ä¿å­˜éŠ€è¡Œæª”æ¡ˆ"""
    try:
        output_dir = "ä¿¡ç”¨å¡è³‡æ–™"
        os.makedirs(output_dir, exist_ok=True)
        filepath = os.path.join(output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"{'='*60}\n")
            f.write(f"  {bank_name} ä¿¡ç”¨å¡è³‡æ–™å½™æ•´\n")
            f.write(f"  çˆ¬å–æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"{'='*60}\n\n")
            
            for i, (card_name, lines) in enumerate(cards_data, 1):
                f.write(f"\n{'-'*20}\n")
                f.write(f"ç¬¬ {i} å¼µå¡ç‰‡: {card_name}\n")
                f.write(f"{'-'*20}\n\n")
                
                for line in lines:
                    f.write(f"{line}\n")
                
                f.write(f"\n{'-'*20}\n")
                f.write(f"{card_name} è³‡æ–™çµæŸ\n")
                f.write(f"{'-'*20}\n\n")
            
            f.write(f"\n{'='*60}\n")
            f.write(f"  {bank_name} è³‡æ–™å½™æ•´å®Œæˆ (å…± {len(cards_data)} å¼µå¡ç‰‡)\n")
            f.write(f"{'='*60}\n")
        
        print(f"ğŸ“ å·²ä¿å­˜ {bank_name} è³‡æ–™åˆ°: {filepath}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜ {bank_name} è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


def lineBank_to_txt_data():
    """ç²å–è¯é‚¦LINE Bankè¯åå¡è³‡æ–™ï¼Œè¿”å›è³‡æ–™è€Œä¸å„²å­˜"""
    url = "https://card.ubot.com.tw/CardDetail/cardDetail601"      
    driver = webdriver.Chrome()     
    driver.get(url)     
    time.sleep(5)      

    soup = BeautifulSoup(driver.page_source, "html.parser")     
    driver.quit()      

    credit_card_name = "è¯é‚¦LINE Bankè¯åå¡"
    
    # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ ï¼ˆå¦‚scriptã€styleæ¨™ç±¤ï¼‰
    for script in soup(["script", "style", "noscript"]):
        script.decompose()
    
    # ç²å–æ•´å€‹é é¢çš„æ‰€æœ‰æ–‡å­—å…§å®¹
    all_text = soup.get_text(separator="\n", strip=True)
    
    # å°‡æ–‡å­—åˆ†è¡Œä¸¦æ¸…ç†
    all_lines = []
    for line in all_text.split('\n'):
        line = line.strip()
        if line:  # åªä¿ç•™éç©ºç™½è¡Œ
            all_lines.append(line)
    
    # å¦‚æœæ²’æœ‰å…§å®¹ï¼Œé¡¯ç¤ºéŒ¯èª¤è¨Šæ¯
    if not all_lines:
        all_lines.append("âš ï¸ ç„¡æ³•æ“·å–å…§å®¹ï¼Œè«‹æª¢æŸ¥é é¢çµæ§‹")
    
    return (credit_card_name, all_lines)

def goodbird_to_txt_data():
    """ç²å–å‰é¶´å¡è³‡æ–™ï¼Œè¿”å›è³‡æ–™è€Œä¸å„²å­˜"""
    url = "https://card.ubot.com.tw/CardDetail/cardDetail202"          
    
    driver = webdriver.Chrome()     
    driver.get(url)     
    time.sleep(5)     
    soup = BeautifulSoup(driver.page_source, "html.parser")     
    driver.quit()           
    
    for script in soup(["script", "style", "noscript"]):         
        script.decompose()          
    
    all_text = soup.get_text(separator="\n", strip=True)     
    credit_card_name = "å‰é¶´å¡"     
    
    all_lines = []     
    for line in all_text.split('\n'):         
        line = line.strip()         
        if line:             
            all_lines.append(line)          
    
    if not all_lines:         
        all_lines.append("âš ï¸ ç„¡æ³•æ“·å–å…§å®¹ï¼Œè«‹æª¢æŸ¥é é¢çµæ§‹")     
    
    return (credit_card_name, all_lines)

def save_both_cards_to_txt(filename="è¯é‚¦éŠ€è¡Œ.txt"):
    """åˆä½µå…©å¼µå¡ç‰‡è³‡æ–™ä¸¦å„²å­˜åˆ°åŒä¸€å€‹æª”æ¡ˆ"""
    print("ğŸ”„ é–‹å§‹æ“·å–è¯é‚¦LINE Bankè¯åå¡è³‡æ–™...")
    line_card_data = lineBank_to_txt_data()
    print(f"âœ… å·²æ“·å– {line_card_data[0]} è³‡æ–™")
    
    print("ğŸ”„ é–‹å§‹æ“·å–å‰é¶´å¡è³‡æ–™...")
    goodbird_card_data = goodbird_to_txt_data()
    print(f"âœ… å·²æ“·å– {goodbird_card_data[0]} è³‡æ–™")
    
    # åˆä½µå…©å¼µå¡ç‰‡çš„è³‡æ–™
    cards_data = [line_card_data, goodbird_card_data]
    
    # ä¸€æ¬¡æ€§å„²å­˜
    save_bank_file("è¯é‚¦éŠ€è¡Œ", cards_data, filename)
    print(f"ğŸ‰ å…©å¼µå¡ç‰‡è³‡æ–™å·²åˆä½µå„²å­˜åˆ° {filename}")


def scrape_taishin_bank():
    """çˆ¬å–å°æ–°éŠ€è¡Œæ‰€æœ‰ä¿¡ç”¨å¡"""
    print(f"\nğŸ¦ é–‹å§‹è™•ç†å°æ–°éŠ€è¡Œ...")
    cards_data = [
        ("å°æ–°ä¿¡ç”¨å¡", scrape_taishin_card())
    ]
    save_bank_file("å°æ–°éŠ€è¡Œ", cards_data, "å°æ–°éŠ€è¡Œ.txt")
    print("âœ… å°æ–°éŠ€è¡Œè™•ç†å®Œæˆ!")

def scrape_sinopac_bank():
    """çˆ¬å–æ°¸è±éŠ€è¡Œæ‰€æœ‰ä¿¡ç”¨å¡"""
    print(f"\nğŸ¦ é–‹å§‹è™•ç†æ°¸è±éŠ€è¡Œ...")
    cards_data = [
        ("DAWHOå¡", scrape_sinopac_dawho_card()),
        ("DAWAYå¡", scrape_sinopac_daway_card())
    ]
    save_bank_file("æ°¸è±éŠ€è¡Œ", cards_data, "æ°¸è±éŠ€è¡Œ.txt")
    print("âœ… æ°¸è±éŠ€è¡Œè™•ç†å®Œæˆ!")

def scrape_hsbc_bank():
    """çˆ¬å–åŒ¯è±éŠ€è¡Œæ‰€æœ‰ä¿¡ç”¨å¡"""
    print(f"\nğŸ¦ é–‹å§‹è™•ç†åŒ¯è±éŠ€è¡Œ...")
    cards_data = [
        ("ç¾é‡‘å›é¥‹å¾¡ç’½å¡", scrape_hsbc_cashback_card()),
        ("LivePlusæ‚ æ´»å¡", scrape_hsbc_liveplus_card())
    ]
    save_bank_file("åŒ¯è±éŠ€è¡Œ", cards_data, "åŒ¯è±éŠ€è¡Œ.txt")
    print("âœ… åŒ¯è±éŠ€è¡Œè™•ç†å®Œæˆ!")

def scrape_esun_bank():
    """çˆ¬å–ç‰å±±éŠ€è¡Œæ‰€æœ‰ä¿¡ç”¨å¡"""
    print(f"\nğŸ¦ é–‹å§‹è™•ç†ç‰å±±éŠ€è¡Œ...")
    
    # å…ˆçˆ¬å– U Bear å¡åŸºæœ¬è³‡æ–™
    unicard_data = scrape_esun_unicard()
    
    # å†çˆ¬å–ç™¾å¤§åº—å®¶è³‡æ–™
    top100_data = scrape_esun_unicard_top100()
    
    # åˆä½µ U Bear å¡çš„å…©éƒ¨åˆ†è³‡æ–™
    combined_unicard_data = unicard_data + top100_data
    
    cards_data = [
        ("PiéŒ¢åŒ…ä¿¡ç”¨å¡", scrape_esun_pi_card()),
        ("U Bearå¡(å«ç™¾å¤§åº—å®¶)", combined_unicard_data),
        ("ç†Šæœ¬ç†Šä¿¡ç”¨å¡", scrape_esun_kumamon_card())
    ]
    save_bank_file("ç‰å±±éŠ€è¡Œ", cards_data, "ç‰å±±éŠ€è¡Œ.txt")
    print("âœ… ç‰å±±éŠ€è¡Œè™•ç†å®Œæˆ!")

def scrape_cathay_bank():
    """çˆ¬å–åœ‹æ³°ä¸–è¯éŠ€è¡Œæ‰€æœ‰ä¿¡ç”¨å¡"""
    print(f"\nğŸ¦ é–‹å§‹è™•ç†åœ‹æ³°ä¸–è¯éŠ€è¡Œ...")
    cards_data = [
        ("CUBEå¡", scrape_cathay_cube_card()),
    ]
    save_bank_file("åœ‹æ³°ä¸–è¯éŠ€è¡Œ", cards_data, "åœ‹æ³°ä¸–è¯éŠ€è¡Œ.txt")
    url = "https://www.cathay-cube.com.tw/cathaybk/personal/product/credit-card/cards/cube-list"
        
    driver = webdriver.Chrome()
    driver.get(url)
    time.sleep(5)
    try:
        print(f"ğŸ” æ­£åœ¨çˆ¬å–: åœ‹æ³°ä¸–è¯éŠ€è¡Œä¿¡ç”¨å¡æ¬Šç›Š")
        driver.get(url)
        time.sleep(10)

        soup = BeautifulSoup(driver.page_source, "html.parser")

        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for tag in soup(["script", "style", "noscript", "nav", "footer", "header"]):
            tag.decompose()

        # æ“·å–æ•´é æ–‡å­—
        all_text = soup.get_text(separator="\n", strip=True)
        all_lines = []
        for line in all_text.split('\n'):
            line = line.strip()
            if line and len(line) > 1 and not line.isspace():
                all_lines.append(line)

        # ğŸ”‘ æ“·å– a class="last:border-transparent" çš„å…§å®¹
        links = soup.find_all("a", class_="last:border-transparent")
        if links:
            all_lines.append("\n--- cubeå¡æ¬Šç›Š ---")
            for link in links:
                text = link.get_text(strip=True)
                href = link.get("href", "")
                if text:
                    all_lines.append(f"{text}")

        if not all_lines:
            all_lines.append(f"âš ï¸ ç„¡æ³•æ“·å–å…§å®¹: {url}")

        print(f"âœ… æˆåŠŸçˆ¬å– åœ‹æ³°ä¸–è¯ä¿¡ç”¨å¡æ¬Šç›Š: {len(all_lines)} è¡Œå…§å®¹")

        # å­˜æª”
        cards_data = [
            ("åœ‹æ³°ä¸–è¯ä¿¡ç”¨å¡æ¬Šç›Š", all_lines)
        ]
        save_bank_file("åœ‹æ³°ä¸–è¯éŠ€è¡Œ", cards_data, "åœ‹æ³°ä¸–è¯éŠ€è¡Œ.txt")
        print(f"ğŸ“ å·²ä¿å­˜ åœ‹æ³°ä¸–è¯ä¿¡ç”¨å¡æ¬Šç›Š åˆ° {"åœ‹æ³°ä¸–è¯éŠ€è¡Œ"}")

        return all_lines

    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±æ•— åœ‹æ³°ä¸–è¯ä¿¡ç”¨å¡æ¬Šç›Š: {e}")
        return [f"éŒ¯èª¤: ç„¡æ³•çˆ¬å– åœ‹æ³°ä¸–è¯ä¿¡ç”¨å¡æ¬Šç›Š", f"ç¶²å€: {url}", f"éŒ¯èª¤è¨Šæ¯: {str(e)}"]
    finally:
        driver.quit()

        return all_lines

    print("âœ… åœ‹æ³°ä¸–è¯éŠ€è¡Œè™•ç†å®Œæˆ!")


def scrape_all_banks():
    """çˆ¬å–æ‰€æœ‰éŠ€è¡Œçš„ä¿¡ç”¨å¡"""
    print("é–‹å§‹çˆ¬å–æ‰€æœ‰éŠ€è¡Œä¿¡ç”¨å¡è³‡æ–™...\n")
    start_time = time.time()
    
    # åŸ·è¡Œæ‰€æœ‰éŠ€è¡Œçˆ¬å–
    
    scrape_taishin_bank()
    time.sleep(3)
    
    scrape_sinopac_bank()
    time.sleep(3)
    
    scrape_hsbc_bank()
    time.sleep(3)
    
    scrape_esun_bank()
    time.sleep(3)
    
    scrape_cathay_bank()
    time.sleep(3)

    # è¯é‚¦çš„å…©å¼µ
    save_both_cards_to_txt()
    time.sleep(3)

    end_time = time.time()
    total_time = int(end_time - start_time)
    
    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆ! ç¸½è€—æ™‚: {total_time//60}åˆ†{total_time%60}ç§’")
    print(f"ğŸ“‚ æ‰€æœ‰è³‡æ–™å·²ä¿å­˜åˆ° 'ä¿¡ç”¨å¡è³‡æ–™' è³‡æ–™å¤¾ä¸­")


# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    # çˆ¬å–æ‰€æœ‰éŠ€è¡Œ
    scrape_all_banks()
    
    # æˆ–è€…åªçˆ¬å–ç‰¹å®šéŠ€è¡Œ
    # scrape_esun_bank()      
